---
title: Envoy Integration
description: How Plano extends Envoy with Proxy-WASM filters
---

## Overview

Plano is built on **Envoy proxy** using the [Proxy-WASM](https://github.com/proxy-wasm/spec) standard. This allows Plano to leverage Envoy's mature HTTP infrastructure while adding agentic intelligence through WASM filters.

<Info>
  Envoy handles all transport-layer concerns (TLS, HTTP/2, connection pooling, retries) so Plano never reimplements these complex systems.
</Info>

## Why Proxy-WASM?

Proxy-WASM is a specification for embedding WebAssembly modules in proxies like Envoy:

<CardGroup cols={2}>
  <Card title="In-Process Execution" icon="microchip">
    Filters run inside Envoy worker threads - no network hops, no IPC overhead
  </Card>
  
  <Card title="Memory Safety" icon="shield">
    WASM provides sandboxed execution - filter bugs can't crash Envoy
  </Card>
  
  <Card title="Language Agnostic" icon="code">
    Write filters in Rust, C++, Go, or any language that compiles to WASM
  </Card>
  
  <Card title="Hot Reload" icon="rotate">
    Update filters without restarting Envoy (via xDS config updates)
  </Card>
</CardGroup>

## Filter Chain Architecture

Envoy processes requests through a **filter chain** - an ordered list of filters that can inspect and modify requests/responses:

```
Client → Listener → TLS → HTTP Manager → [Filters] → Upstream
                                          ↓
                                    prompt_gateway.wasm
                                    llm_gateway.wasm
                                    envoy.router
```

<Steps>
  <Step title="Listener Filter">
    Pre-TLS processing - SNI inspection, protocol detection
  </Step>
  
  <Step title="Transport Socket">
    TLS termination and decryption
  </Step>
  
  <Step title="HTTP Connection Manager">
    HTTP/1.1 or HTTP/2 codec, header processing
  </Step>
  
  <Step title="Custom WASM Filters">
    `prompt_gateway.wasm` and `llm_gateway.wasm` run here
  </Step>
  
  <Step title="Router Filter">
    Envoy's built-in router - selects upstream cluster and endpoint
  </Step>
</Steps>

## Plano's WASM Filters

### Filter Placement

Plano deploys two WASM filters at different points in the request lifecycle:

<Tabs>
  <Tab title="Ingress (prompt_gateway)">
    **Location:** Downstream HTTP filter chain  
    **Phase:** After TLS termination, before routing  
    **Purpose:** Process incoming prompts and coordinate with brightstaff
    
    Runs on:
    - Agent listener (port 8001+)
    - Function-call listener (port 10000)
    
    **Configuration snippet:**
    ```yaml
    http_filters:
      - name: envoy.filters.http.wasm
        typed_config:
          '@type': type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm
          config:
            vm_config:
              runtime: envoy.wasm.runtime.v8
              code:
                local:
                  filename: /app/prompt_gateway.wasm
    ```
  </Tab>
  
  <Tab title="Egress (llm_gateway)">
    **Location:** Upstream HTTP filter chain  
    **Phase:** Before sending to LLM providers  
    **Purpose:** Format translation and rate limiting
    
    Runs on:
    - Model listener (port 12000)
    - LLM provider clusters
    
    **Configuration snippet:**
    ```yaml
    http_filters:
      - name: envoy.filters.http.wasm
        typed_config:
          '@type': type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm
          config:
            vm_config:
              runtime: envoy.wasm.runtime.v8
              code:
                local:
                  filename: /app/llm_gateway.wasm
    ```
  </Tab>
</Tabs>

### Filter Lifecycle

Proxy-WASM filters implement a defined lifecycle:

```rust
// 1. Root Context (per-filter configuration)
proxy_wasm::set_root_context(|_| -> Box<dyn RootContext> {
    Box::new(FilterContext::new())
});

// 2. HTTP Context (per-request)
impl HttpContext for StreamContext {
    fn on_http_request_headers(&mut self, _: usize, _: bool) -> Action {
        // Inspect/modify request headers
        Action::Continue
    }
    
    fn on_http_request_body(&mut self, body_size: usize, end_of_stream: bool) -> Action {
        // Process request body
        Action::Continue
    }
    
    fn on_http_response_headers(&mut self, _: usize, _: bool) -> Action {
        // Inspect/modify response headers
        Action::Continue
    }
    
    fn on_http_response_body(&mut self, body_size: usize, end_of_stream: bool) -> Action {
        // Process response body
        Action::Continue
    }
}
```

<Note>
  Each worker thread maintains its own filter instances - no cross-thread synchronization required.
</Note>

## Communication with Brightstaff

WASM filters are **sandboxed** and cannot make arbitrary network calls. They communicate with brightstaff via:

### HTTP Callouts

**API:** `proxy_wasm::hostcalls::dispatch_http_call()`

Filters make HTTP requests to brightstaff (running on `localhost:9091`):

```rust
use proxy_wasm::traits::HttpContext;
use proxy_wasm::types::Action;

impl HttpContext for StreamContext {
    fn on_http_request_headers(&mut self, _: usize, _: bool) -> Action {
        // Extract prompt from request
        let prompt = self.get_http_request_header(":path");
        
        // Call brightstaff for routing decision
        let call_id = self.dispatch_http_call(
            "brightstaff_cluster",  // Envoy cluster name
            vec![
                (":method", "POST"),
                (":path", "/route"),
                (":authority", "localhost:9091"),
            ],
            Some(prompt_body.as_bytes()),
            vec![],  // trailers
            Duration::from_secs(5),  // timeout
        ).unwrap();
        
        // Pause request processing until response arrives
        Action::Pause
    }
    
    fn on_http_call_response(&mut self, call_id: u32, _: usize, _: usize, _: usize) {
        // Process brightstaff response
        let response_body = self.get_http_call_response_body(0, usize::MAX).unwrap();
        
        // Update request headers based on routing decision
        self.set_http_request_header("x-selected-agent", agent_id);
        
        // Resume request processing
        self.resume_http_request();
    }
}
```

<Warning>
  HTTP callouts are **synchronous from the filter's perspective** - the filter pauses the request until the response arrives. Keep callouts fast!
</Warning>

### Shared Data

Filters can share data via Envoy's shared memory:

```rust
// Write to shared data
self.set_shared_data(
    "rate_limit_tokens",
    Some(tokens.to_string().as_bytes()),
    None,  // CAS (compare-and-swap) value
).unwrap();

// Read from shared data
if let Some((data, _cas)) = self.get_shared_data("rate_limit_tokens") {
    let tokens = String::from_utf8(data).unwrap();
}
```

<Info>
  Shared data is **per-worker-thread** - each worker has its own copy. For global state, use brightstaff.
</Info>

## Envoy Configuration

**File:** `config/envoy.template.yaml`

Plano generates Envoy configuration from a Jinja2 template:

### Listeners

Define ports and filter chains:

```yaml
listeners:
  - name: agent_listener
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 8001
    filter_chains:
      - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: agent_ingress
              codec_type: AUTO
              http_filters:
                - name: envoy.filters.http.wasm
                  typed_config:
                    '@type': type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm
                    config:
                      name: prompt_gateway
                      vm_config:
                        runtime: envoy.wasm.runtime.v8
                        code:
                          local:
                            filename: /app/prompt_gateway.wasm
                - name: envoy.filters.http.router
                  typed_config:
                    '@type': type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
```

### Clusters

Define upstream endpoints:

```yaml
clusters:
  - name: brightstaff_cluster
    type: STATIC
    connect_timeout: 5s
    load_assignment:
      cluster_name: brightstaff_cluster
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 9091
  
  - name: openai_cluster
    type: LOGICAL_DNS
    dns_lookup_family: V4_ONLY
    connect_timeout: 30s
    load_assignment:
      cluster_name: openai_cluster
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: api.openai.com
                    port_value: 443
    transport_socket:
      name: envoy.transport_sockets.tls
      typed_config:
        '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
        sni: api.openai.com
```

### Access Logs

Structured JSON logging:

```yaml
access_log:
  - name: envoy.access_loggers.file
    typed_config:
      '@type': type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
      path: /var/log/access_agent.log
      log_format:
        json_format:
          timestamp: '%START_TIME%'
          method: '%REQ(:METHOD)%'
          path: '%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%'
          status: '%RESPONSE_CODE%'
          duration_ms: '%DURATION%'
          upstream_service: '%UPSTREAM_CLUSTER%'
          trace_id: '%REQ(TRACEPARENT)%'
```

## Threading Model

Envoy uses an **event-based threading model** that Plano inherits:

### Worker Threads

Each worker thread:

1. Runs its own event loop (libevent)
2. Maintains its own connection pools
3. Executes its own filter instances
4. Processes requests **completely independently**

<Info>
  Default: **1 worker per CPU core**. Configure via `--concurrency N` flag.
</Info>

### Connection Affinity

Each TCP connection is **pinned to one worker** for its lifetime:

- No lock contention between workers
- Cache-friendly (L1/L2 cache per worker)
- Trivially parallel scaling

```
┌─────────────────────────────────────────┐
│             ENVOY PROCESS               │
│                                         │
│  ┌──────────┐  ┌──────────┐  ┌────────┐│
│  │ Worker 1 │  │ Worker 2 │  │ Worker N││
│  │          │  │          │  │        ││
│  │ Event    │  │ Event    │  │ Event  ││
│  │ Loop     │  │ Loop     │  │ Loop   ││
│  │          │  │          │  │        ││
│  │ Filters  │  │ Filters  │  │ Filters││
│  │ Pools    │  │ Pools    │  │ Pools  ││
│  └──────────┘  └──────────┘  └────────┘│
│       ↕              ↕            ↕     │
│   Conn 1-N      Conn N+1...   Conn M   │
└─────────────────────────────────────────┘
```

### Main Thread

Handles:
- Configuration updates (xDS protocol)
- Stats aggregation from workers
- Admin API (`/stats`, `/config_dump`)
- Graceful shutdown coordination

<Tip>
  Workers rarely synchronize - only for shared stats and admin API responses.
</Tip>

## Performance Characteristics

### WASM Overhead

Proxy-WASM filters run with **near-native performance**:

- **V8 runtime** - JIT compilation to native code
- **Zero serialization** - Direct memory access to Envoy buffers
- **No syscalls** - Hostcalls are function calls, not OS syscalls

Typical overhead: **< 1ms per request** for simple filters.

### Memory Isolation

Each filter instance has its own WASM heap:

- **Per-worker isolation** - Worker 1's filter can't access Worker 2's memory
- **Bounded memory** - WASM heap size configurable (default: 2MB)
- **Garbage collection** - WASM heap freed when filter instance destroyed

### Streaming

Filters can process streaming responses **incrementally**:

```rust
fn on_http_response_body(&mut self, body_size: usize, end_of_stream: bool) -> Action {
    if !end_of_stream {
        // Process chunk
        let chunk = self.get_http_response_body(0, body_size).unwrap();
        // ... extract tokens, measure TTFT ...
        
        // Pass through to client immediately
        Action::Continue
    } else {
        // Final chunk - finalize metrics
        Action::Continue
    }
}
```

<Info>
  No buffering required - stream processing enables low latency for LLM streaming responses.
</Info>

## Debugging WASM Filters

### Logging

Use `log!` macro from `proxy-wasm`:

```rust
use proxy_wasm::hostcalls::log;
use proxy_wasm::types::LogLevel;

log(LogLevel::Info, &format!("Processing request to {}", path));
log(LogLevel::Warn, "Rate limit approaching threshold");
log(LogLevel::Error, &format!("Failed to parse: {}", err));
```

Logs appear in Envoy output with `[wasm]` prefix:

```
[2026-02-28 10:00:00.123][info][wasm] Processing request to /v1/chat/completions
```

### Admin API

Envoy's admin interface (port 9901 by default) provides:

- `/stats` - Runtime statistics from filters
- `/config_dump` - Current Envoy configuration
- `/clusters` - Upstream cluster health
- `/runtime` - Runtime feature flags

```bash
curl http://localhost:9901/stats | grep wasm
curl http://localhost:9901/config_dump
```

### Local Development

Test filters outside of Envoy:

```bash
# Build WASM filter
cd crates
cargo build --target=wasm32-wasip1 -p llm_gateway

# Run unit tests (without Envoy)
cargo test -p llm_gateway

# Run E2E tests (with Envoy in Docker)
cd ..
./tests/e2e/run_e2e_tests.sh
```

## Configuration Management

### Template Rendering

**File:** `cli/planoai/config_generator.py`

The `planoai up` command:

1. Validates `plano_config.yaml` against JSON schema
2. Renders `config/envoy.template.yaml` with Jinja2
3. Substitutes environment variables with `envsubst`
4. Writes final config to `/etc/envoy/envoy.yaml`

### Dynamic Configuration (xDS)

Plano currently uses **static configuration** but can be extended to use Envoy's xDS APIs:

- **LDS** (Listener Discovery Service) - Dynamic listener updates
- **CDS** (Cluster Discovery Service) - Dynamic upstream updates
- **RDS** (Route Discovery Service) - Dynamic routing rules
- **EDS** (Endpoint Discovery Service) - Dynamic endpoint health

<Note>
  Future versions may support xDS for zero-downtime configuration updates.
</Note>

## Extending Plano

### Adding a New Filter

<Steps>
  <Step title="Create Rust Crate">
    ```bash
    cd crates
    cargo new my_filter --lib
    ```
    
    Update `Cargo.toml`:
    ```toml
    [lib]
    crate-type = ["cdylib"]
    
    [dependencies]
    proxy-wasm = "0.2.1"
    ```
  </Step>
  
  <Step title="Implement Proxy-WASM Traits">
    ```rust
    use proxy_wasm::traits::*;
    use proxy_wasm::types::*;
    
    proxy_wasm::main! {{
        proxy_wasm::set_root_context(|_| -> Box<dyn RootContext> {
            Box::new(MyFilterContext::new())
        });
    }}
    
    struct MyFilterContext;
    
    impl Context for MyFilterContext {}
    impl RootContext for MyFilterContext {}
    impl HttpContext for MyFilterContext {
        // ... implement filter logic ...
    }
    ```
  </Step>
  
  <Step title="Build WASM Module">
    ```bash
    cargo build --release --target=wasm32-wasip1 -p my_filter
    ```
  </Step>
  
  <Step title="Update Envoy Config">
    Add filter to `config/envoy.template.yaml`:
    ```yaml
    http_filters:
      - name: envoy.filters.http.wasm
        typed_config:
          config:
            vm_config:
              code:
                local:
                  filename: /app/my_filter.wasm
    ```
  </Step>
</Steps>

## Best Practices

<CardGroup cols={2}>
  <Card title="Keep Filters Fast" icon="bolt">
    Minimize processing in filter callbacks - offload heavy work to brightstaff via HTTP callouts
  </Card>
  
  <Card title="Handle Errors Gracefully" icon="circle-exclamation">
    Always check return values from hostcalls - they can fail if Envoy is shutting down
  </Card>
  
  <Card title="Avoid Shared State" icon="ban">
    Use per-worker state when possible - shared data requires synchronization
  </Card>
  
  <Card title="Test Thoroughly" icon="flask">
    Write unit tests and E2E tests - filter bugs can affect all requests
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Configuration" icon="gear" href="/configuration/overview">
    Learn how to configure listeners and clusters
  </Card>
  
  <Card title="Development Setup" icon="code" href="/development/setup">
    Set up a local development environment
  </Card>
  
  <Card title="Observability" icon="chart-line" href="/observability/tracing">
    Configure tracing and metrics
  </Card>
  
  <Card title="Proxy-WASM Spec" icon="book" href="https://github.com/proxy-wasm/spec">
    Read the official Proxy-WASM specification
  </Card>
</CardGroup>
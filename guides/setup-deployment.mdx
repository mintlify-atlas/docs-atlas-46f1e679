---
title: Setup & Deployment
description: Install Plano, configure your first project, and deploy with Docker in production environments
---

Plano is distributed as a Docker container that bundles Envoy proxy, WASM plugins, and the brightstaff binary into a single deployable unit. This guide walks you through local setup, Docker deployment, and production configuration.

## Prerequisites

Before starting, ensure you have:

- **Docker** (version 20.10 or later)
- **Python 3.9+** for the Plano CLI
- **API Keys** for your LLM providers (OpenAI, Anthropic, etc.)

<Note>
Plano is backed by industry-leading LLM research and built on [Envoy](https://envoyproxy.io) by its core contributors.
</Note>

## Installing the Plano CLI

The `planoai` CLI manages the complete Plano lifecycle - from validation to container management.

<Steps>

<Step title="Install via pip">
Install the CLI globally:

```bash
pip install planoai
```

Verify installation:

```bash
planoai --version
```
</Step>

<Step title="Pull the Docker image">
The CLI automatically pulls the latest Plano image, but you can do it manually:

```bash
docker pull katanemo/plano:latest
```
</Step>

<Step title="Set up environment variables">
Create a `.env` file or export your API keys:

```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```
</Step>

</Steps>

## Creating Your First Configuration

Plano uses a YAML configuration file to define agents, model providers, listeners, and routing behavior.

### Basic LLM Gateway Configuration

Start with a simple LLM routing setup:

```yaml config.yaml
version: v0.3.0

model_providers:
  - model: openai/gpt-5.2
    access_key: $OPENAI_API_KEY
    default: true
  
  - model: anthropic/claude-sonnet-4-5
    access_key: $ANTHROPIC_API_KEY

listeners:
  - type: model
    name: llm_gateway
    port: 12000
    timeout: 30s

tracing:
  random_sampling: 100
```

<Info>
The `listeners` section defines ingress points. Use `type: model` for direct LLM routing, or `type: agent` for multi-agent orchestration.
</Info>

### Multi-Agent Configuration

For agentic applications, define agents and routing logic:

```yaml config.yaml
version: v0.3.0

agents:
  - id: weather_agent
    url: http://host.docker.internal:10510
  - id: flight_agent
    url: http://host.docker.internal:10520

model_providers:
  - model: openai/gpt-5.2
    access_key: $OPENAI_API_KEY
    default: true
  - model: openai/gpt-4o-mini
    access_key: $OPENAI_API_KEY

listeners:
  - type: agent
    name: travel_assistant
    port: 8001
    router: plano_orchestrator_v1
    agents:
      - id: weather_agent
        description: |
          Gets real-time weather and forecasts for any city worldwide.
          Handles: "What's the weather in Paris?", "Will it rain in Tokyo?"
      
      - id: flight_agent
        description: |
          Searches flights between airports with live status and schedules.
          Handles: "Flights from NYC to LA", "Show me flights to Seattle"

tracing:
  random_sampling: 100
```

## Starting Plano

### Using the CLI (Recommended)

The `planoai up` command validates your config and starts the container:

```bash
planoai up config.yaml
```

You'll see output like:

```
✓ Configuration validated
✓ API keys verified
✓ Starting Plano container...
✓ Plano is running on ports 8001, 12000
```

<Tip>
Use `planoai up --with-tracing` to automatically start the local OTLP listener for traces.
</Tip>

### View logs

Stream access and debug logs:

```bash
planoai logs
```

### Stop Plano

Cleanly shutdown the container:

```bash
planoai down
```

## Docker Deployment

### Using Docker Compose

For production deployments with multiple services, use Docker Compose:

```yaml docker-compose.yaml
services:
  plano:
    image: katanemo/plano:latest
    ports:
      - "8001:8001"    # Agent listener
      - "12000:12000"  # Model gateway
      - "19901:19901"  # Prometheus metrics
    volumes:
      - ./config.yaml:/app/plano_config.yaml
      - /etc/ssl/cert.pem:/etc/ssl/cert.pem
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OTEL_TRACING_GRPC_ENDPOINT=http://otel-collector:4317
    extra_hosts:
      - "host.docker.internal:host-gateway"

  weather-agent:
    build: ./agents/weather
    ports:
      - "10510:10510"
    environment:
      - LLM_GATEWAY_ENDPOINT=http://host.docker.internal:12000/v1
    extra_hosts:
      - "host.docker.internal:host-gateway"

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "4317:4317"    # OTLP gRPC
```

Start all services:

```bash
docker compose up -d
```

### Building Custom Images

To build from source with custom WASM filters:

```dockerfile Dockerfile
# Multi-stage build for custom Plano image
FROM rust:1.93.0 AS wasm-builder
RUN rustup target add wasm32-wasip1

WORKDIR /arch
COPY crates/ ./

# Build WASM plugins
RUN cargo build --release --target wasm32-wasip1 -p prompt_gateway -p llm_gateway

# Final image
FROM katanemo/plano:latest
COPY --from=wasm-builder /arch/target/wasm32-wasip1/release/prompt_gateway.wasm /etc/envoy/proxy-wasm-plugins/
COPY --from=wasm-builder /arch/target/wasm32-wasip1/release/llm_gateway.wasm /etc/envoy/proxy-wasm-plugins/
```

Build and run:

```bash
docker build -t plano-custom:latest .
docker run -p 8001:8001 -p 12000:12000 \
  -v ./config.yaml:/app/plano_config.yaml \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  plano-custom:latest
```

## Production Configuration

### Environment Variables

Plano supports these environment variables:

| Variable | Description | Default |
|----------|-------------|----------|
| `PLANO_CONFIG_PATH` | Path to config file | `/app/plano_config.yaml` |
| `OTEL_TRACING_GRPC_ENDPOINT` | OpenTelemetry collector endpoint | None |
| `LOG_LEVEL` | Logging verbosity | `info` |

### Port Configuration

Plano exposes these ports:

| Port | Purpose |
|------|----------|
| `8001` | Agent orchestration listener |
| `12000` | LLM gateway listener |
| `19901` | Prometheus metrics endpoint |
| `9901` | Envoy admin interface |

<Warning>
Never expose port 9901 publicly - it provides direct access to Envoy's admin interface.
</Warning>

### Health Checks

Configure Docker health checks:

```yaml docker-compose.yaml
services:
  plano:
    image: katanemo/plano:latest
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9901/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
```

### Resource Limits

Set resource constraints for production:

```yaml docker-compose.yaml
services:
  plano:
    image: katanemo/plano:latest
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
```

## Configuration Validation

Plano validates configurations against a JSON Schema before starting.

### Validate manually

```bash
# Using the validation script
bash config/validate_plano_config.sh config.yaml

# Using the CLI (implicit validation)
planoai up config.yaml --dry-run
```

### Schema Reference

The configuration schema is defined in `plano_config_schema.yaml`:

```yaml
version:
  type: string
  enum: [v0.1, v0.1.0, v0.2.0, v0.3.0]

agents:
  type: array
  items:
    properties:
      id: {type: string}
      url: {type: string}
    required: [id, url]

listeners:
  type: array
  items:
    properties:
      type: {enum: [agent, model, prompt]}
      name: {type: string}
      port: {type: integer}
      router: {enum: [plano_orchestrator_v1]}
```

## Troubleshooting

### Container won't start

<AccordionGroup>
  <Accordion title="Check Docker logs">
    ```bash
    docker logs plano-container
    ```
  </Accordion>
  
  <Accordion title="Verify config syntax">
    ```bash
    planoai up config.yaml --dry-run
    ```
  </Accordion>
  
  <Accordion title="Ensure ports are available">
    ```bash
    lsof -i :8001
    lsof -i :12000
    ```
  </Accordion>
</AccordionGroup>

### API keys not working

Verify environment variables are set:

```bash
docker exec plano-container env | grep API_KEY
```

### Connection refused errors

When agents can't reach Plano, use `host.docker.internal`:

```yaml
agents:
  - id: my_agent
    url: http://host.docker.internal:10500  # Not localhost!
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Build Agents" icon="robot" href="/guides/building-agents">
    Learn how to implement agents that integrate with Plano
  </Card>
  
  <Card title="LLM Gateway" icon="route" href="/guides/llm-gateway">
    Configure model routing and provider management
  </Card>
  
  <Card title="Tracing" icon="chart-line" href="/guides/tracing-monitoring">
    Set up OpenTelemetry tracing for observability
  </Card>
  
  <Card title="Guardrails" icon="shield-halved" href="/guides/guardrails">
    Add safety and validation filters to your agents
  </Card>
</CardGroup>